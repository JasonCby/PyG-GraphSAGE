{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Citeseer()\n"
     ]
    }
   ],
   "source": [
    "dataset_cora = Planetoid(root='./citeseer/', name='Citeseer')\n",
    "# dataset = Planetoid(root='./citeseer',name='Citeseer')\n",
    "# dataset = Planetoid(root='./pubmed/',name='Pubmed')\n",
    "print(dataset_cora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset_cora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from torch_geometric.nn import GATConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/mnt/NVme/project_moka/pubmed/\"\n",
    "path = \"/mnt/ramfs/project_moka/pubmed/\"\n",
    "#path = \"/mnt/ext4ramdisk/project_moka/pubmed/\"\n",
    "#path = \"./pubmed/\"\n",
    "\n",
    "path_Cora = \"/mnt/NVme/project_moka/data/Cora/\"\n",
    "# path_Cora = \"/mnt/ramfs/project_moka/data/Cora/\"\n",
    "#path_Cora = \"/mnt/ext4ramdisk/project_moka/data/Cora/\"\n",
    "#path_Cora = \"./data/Cora/\"\n",
    "\n",
    "path_pm = \"/mnt/NVme/datasets/\"\n",
    "path_ram = \"/mnt/ramfs/datasets/\"\n",
    "\n",
    "times = 15\n",
    "total_time = 0\n",
    "total_run_time = 0\n",
    "batch_size = 128\n",
    "epoch_num = 20\n",
    "\n",
    "# pre-load Planetoid\n",
    "dataset_test = Planetoid(root=\"./pubmed/\", name='Pubmed')\n",
    "#dataset_test = Planetoid(root=path_Cora, name='Cora')\n",
    "# the dataset for test is shown below (different from the above)\n",
    "# dataset_test = Planetoid(root='./data/Cora/', name='Cora')\n",
    "\n",
    "\n",
    "dataset = dataset_test\n",
    "\n",
    "\n",
    "data = dataset[0]  # Get the first graph object.\n",
    "\n",
    "from torch_geometric.data import ClusterData, ClusterLoader, DataLoader\n",
    "\n",
    "torch.manual_seed(32322)\n",
    "cluster_data = ClusterData(data, num_parts=128)  # 1. Create subgraphs.\n",
    "train_loader = ClusterLoader(cluster_data, batch_size=batch_size,\n",
    "                             shuffle=True)  # 2. Stochastic partitioning scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAGENet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SAGENet, self).__init__()\n",
    "        self.conv1 = SAGEConv(dataset.num_node_features, 128)\n",
    "        self.conv2 = SAGEConv(128, dataset.num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.softmax(x, dim=1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GATNet, self).__init__()\n",
    "        self.gat1 = GATConv(dataset.num_node_features, 8, 8, dropout=0.6)\n",
    "        self.gat2 = GATConv(64, 7, 1, dropout=0.6)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.gat1(x, edge_index)\n",
    "        x = self.gat2(x, edge_index)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GCNNet, self).__init__()\n",
    "        self.conv1 = GCNConv(dataset.num_node_features, 16)\n",
    "        self.conv2 = GCNConv(16, dataset.num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAGENet(\n",
      "  (conv1): SAGEConv(3703, 128)\n",
      "  (conv2): SAGEConv(128, 6)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = SAGENet()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GATNet(\n",
      "  (gat1): GATConv(3703, 8, heads=8)\n",
      "  (gat2): GATConv(64, 7, heads=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = GATNet()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCNNet(\n",
      "  (conv1): GCNConv(3703, 16)\n",
      "  (conv2): GCNConv(16, 6)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = GCNNet()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "Data(x=[3327, 3703], edge_index=[2, 9104], y=[3327], train_mask=[3327], val_mask=[3327], test_mask=[3327])\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "model.to(device)\n",
    "data = dataset_cora[0].to(device)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000 train_loss: 1.9601 train_acc: 0.1333\n",
      "Epoch 001 train_loss: 1.4059 train_acc: 0.6083\n",
      "Epoch 002 train_loss: 1.1079 train_acc: 0.6917\n",
      "Epoch 003 train_loss: 0.8118 train_acc: 0.7250\n",
      "Epoch 004 train_loss: 0.7000 train_acc: 0.7833\n",
      "Epoch 005 train_loss: 0.7231 train_acc: 0.7750\n",
      "Epoch 006 train_loss: 0.5683 train_acc: 0.8083\n",
      "Epoch 007 train_loss: 0.5700 train_acc: 0.8333\n",
      "Epoch 008 train_loss: 0.4755 train_acc: 0.8500\n",
      "Epoch 009 train_loss: 0.5769 train_acc: 0.7833\n",
      "Epoch 010 train_loss: 0.5165 train_acc: 0.7750\n",
      "Epoch 011 train_loss: 0.4711 train_acc: 0.8250\n",
      "Epoch 012 train_loss: 0.5539 train_acc: 0.7917\n",
      "Epoch 013 train_loss: 0.4473 train_acc: 0.8250\n",
      "Epoch 014 train_loss: 0.4740 train_acc: 0.8250\n",
      "Epoch 015 train_loss: 0.4303 train_acc: 0.8250\n",
      "Epoch 016 train_loss: 0.5084 train_acc: 0.7750\n",
      "Epoch 017 train_loss: 0.3638 train_acc: 0.8333\n",
      "Epoch 018 train_loss: 0.3882 train_acc: 0.8250\n",
      "Epoch 019 train_loss: 0.5298 train_acc: 0.8333\n",
      "Epoch 020 train_loss: 0.4319 train_acc: 0.8083\n",
      "Epoch 021 train_loss: 0.4305 train_acc: 0.8583\n",
      "Epoch 022 train_loss: 0.3512 train_acc: 0.8583\n",
      "Epoch 023 train_loss: 0.3646 train_acc: 0.8417\n",
      "Epoch 024 train_loss: 0.5344 train_acc: 0.7750\n",
      "Epoch 025 train_loss: 0.4884 train_acc: 0.7917\n",
      "Epoch 026 train_loss: 0.4126 train_acc: 0.8083\n",
      "Epoch 027 train_loss: 0.4020 train_acc: 0.8333\n",
      "Epoch 028 train_loss: 0.5700 train_acc: 0.7917\n",
      "Epoch 029 train_loss: 0.4509 train_acc: 0.8000\n",
      "Epoch 030 train_loss: 0.4729 train_acc: 0.7917\n",
      "Epoch 031 train_loss: 0.3982 train_acc: 0.8417\n",
      "Epoch 032 train_loss: 0.4225 train_acc: 0.8417\n",
      "Epoch 033 train_loss: 0.5810 train_acc: 0.7667\n",
      "Epoch 034 train_loss: 0.5030 train_acc: 0.7917\n",
      "Epoch 035 train_loss: 0.4638 train_acc: 0.8000\n",
      "Epoch 036 train_loss: 0.5036 train_acc: 0.7833\n",
      "Epoch 037 train_loss: 0.5156 train_acc: 0.7667\n",
      "Epoch 038 train_loss: 0.4128 train_acc: 0.8250\n",
      "Epoch 039 train_loss: 0.4353 train_acc: 0.8250\n",
      "Epoch 040 train_loss: 0.5285 train_acc: 0.7917\n",
      "Epoch 041 train_loss: 0.4551 train_acc: 0.8167\n",
      "Epoch 042 train_loss: 0.6018 train_acc: 0.8083\n",
      "Epoch 043 train_loss: 0.4278 train_acc: 0.8750\n",
      "Epoch 044 train_loss: 0.5120 train_acc: 0.7833\n",
      "Epoch 045 train_loss: 0.5091 train_acc: 0.7750\n",
      "Epoch 046 train_loss: 0.5306 train_acc: 0.8000\n",
      "Epoch 047 train_loss: 0.4707 train_acc: 0.7917\n",
      "Epoch 048 train_loss: 0.4128 train_acc: 0.8417\n",
      "Epoch 049 train_loss: 0.4969 train_acc: 0.8167\n",
      "Epoch 050 train_loss: 0.4916 train_acc: 0.8000\n",
      "Epoch 051 train_loss: 0.5399 train_acc: 0.7833\n",
      "Epoch 052 train_loss: 0.5066 train_acc: 0.7917\n",
      "Epoch 053 train_loss: 0.4569 train_acc: 0.8083\n",
      "Epoch 054 train_loss: 0.4217 train_acc: 0.8250\n",
      "Epoch 055 train_loss: 0.5068 train_acc: 0.8000\n",
      "Epoch 056 train_loss: 0.4706 train_acc: 0.8250\n",
      "Epoch 057 train_loss: 0.4050 train_acc: 0.8417\n",
      "Epoch 058 train_loss: 0.4832 train_acc: 0.7917\n",
      "Epoch 059 train_loss: 0.3845 train_acc: 0.8333\n",
      "Epoch 060 train_loss: 0.4180 train_acc: 0.8667\n",
      "Epoch 061 train_loss: 0.3668 train_acc: 0.8667\n",
      "Epoch 062 train_loss: 0.3957 train_acc: 0.8417\n",
      "Epoch 063 train_loss: 0.5440 train_acc: 0.8000\n",
      "Epoch 064 train_loss: 0.4854 train_acc: 0.8167\n",
      "Epoch 065 train_loss: 0.4384 train_acc: 0.8250\n",
      "Epoch 066 train_loss: 0.4754 train_acc: 0.8000\n",
      "Epoch 067 train_loss: 0.5002 train_acc: 0.8000\n",
      "Epoch 068 train_loss: 0.5313 train_acc: 0.8000\n",
      "Epoch 069 train_loss: 0.4051 train_acc: 0.8333\n",
      "Epoch 070 train_loss: 0.5209 train_acc: 0.8000\n",
      "Epoch 071 train_loss: 0.5845 train_acc: 0.7583\n",
      "Epoch 072 train_loss: 0.3501 train_acc: 0.8500\n",
      "Epoch 073 train_loss: 0.4148 train_acc: 0.8167\n",
      "Epoch 074 train_loss: 0.3726 train_acc: 0.8417\n",
      "Epoch 075 train_loss: 0.3755 train_acc: 0.8250\n",
      "Epoch 076 train_loss: 0.4598 train_acc: 0.8083\n",
      "Epoch 077 train_loss: 0.4247 train_acc: 0.8250\n",
      "Epoch 078 train_loss: 0.4613 train_acc: 0.8083\n",
      "Epoch 079 train_loss: 0.4096 train_acc: 0.8167\n",
      "Epoch 080 train_loss: 0.5013 train_acc: 0.7833\n",
      "Epoch 081 train_loss: 0.4678 train_acc: 0.8333\n",
      "Epoch 082 train_loss: 0.4528 train_acc: 0.8083\n",
      "Epoch 083 train_loss: 0.4830 train_acc: 0.7833\n",
      "Epoch 084 train_loss: 0.5204 train_acc: 0.7833\n",
      "Epoch 085 train_loss: 0.4812 train_acc: 0.8000\n",
      "Epoch 086 train_loss: 0.3204 train_acc: 0.8750\n",
      "Epoch 087 train_loss: 0.4352 train_acc: 0.8333\n",
      "Epoch 088 train_loss: 0.4071 train_acc: 0.8333\n",
      "Epoch 089 train_loss: 0.3751 train_acc: 0.8667\n",
      "Epoch 090 train_loss: 0.4333 train_acc: 0.8250\n",
      "Epoch 091 train_loss: 0.3193 train_acc: 0.8750\n",
      "Epoch 092 train_loss: 0.4632 train_acc: 0.7833\n",
      "Epoch 093 train_loss: 0.3439 train_acc: 0.8583\n",
      "Epoch 094 train_loss: 0.3815 train_acc: 0.8750\n",
      "Epoch 095 train_loss: 0.4772 train_acc: 0.8167\n",
      "Epoch 096 train_loss: 0.4157 train_acc: 0.8250\n",
      "Epoch 097 train_loss: 0.3528 train_acc: 0.8333\n",
      "Epoch 098 train_loss: 0.5911 train_acc: 0.7667\n",
      "Epoch 099 train_loss: 0.3729 train_acc: 0.8667\n",
      "Epoch 100 train_loss: 0.4823 train_acc: 0.8250\n",
      "Epoch 101 train_loss: 0.4845 train_acc: 0.8000\n",
      "Epoch 102 train_loss: 0.3466 train_acc: 0.8500\n",
      "Epoch 103 train_loss: 0.4621 train_acc: 0.8167\n",
      "Epoch 104 train_loss: 0.4952 train_acc: 0.8083\n",
      "Epoch 105 train_loss: 0.5037 train_acc: 0.7917\n",
      "Epoch 106 train_loss: 0.4538 train_acc: 0.8083\n",
      "Epoch 107 train_loss: 0.4690 train_acc: 0.7917\n",
      "Epoch 108 train_loss: 0.5478 train_acc: 0.7667\n",
      "Epoch 109 train_loss: 0.2934 train_acc: 0.8833\n",
      "Epoch 110 train_loss: 0.4436 train_acc: 0.8333\n",
      "Epoch 111 train_loss: 0.4733 train_acc: 0.7917\n",
      "Epoch 112 train_loss: 0.4301 train_acc: 0.8167\n",
      "Epoch 113 train_loss: 0.4209 train_acc: 0.8333\n",
      "Epoch 114 train_loss: 0.4630 train_acc: 0.8250\n",
      "Epoch 115 train_loss: 0.4285 train_acc: 0.8250\n",
      "Epoch 116 train_loss: 0.3725 train_acc: 0.8333\n",
      "Epoch 117 train_loss: 0.4217 train_acc: 0.8167\n",
      "Epoch 118 train_loss: 0.3514 train_acc: 0.8583\n",
      "Epoch 119 train_loss: 0.4570 train_acc: 0.8000\n",
      "Epoch 120 train_loss: 0.3928 train_acc: 0.8333\n",
      "Epoch 121 train_loss: 0.4323 train_acc: 0.8167\n",
      "Epoch 122 train_loss: 0.5262 train_acc: 0.7750\n",
      "Epoch 123 train_loss: 0.4533 train_acc: 0.8000\n",
      "Epoch 124 train_loss: 0.3802 train_acc: 0.8417\n",
      "Epoch 125 train_loss: 0.4155 train_acc: 0.8500\n",
      "Epoch 126 train_loss: 0.3139 train_acc: 0.8750\n",
      "Epoch 127 train_loss: 0.4083 train_acc: 0.8167\n",
      "Epoch 128 train_loss: 0.4517 train_acc: 0.8000\n",
      "Epoch 129 train_loss: 0.3850 train_acc: 0.8333\n",
      "Epoch 130 train_loss: 0.3315 train_acc: 0.8583\n",
      "Epoch 131 train_loss: 0.6030 train_acc: 0.7500\n",
      "Epoch 132 train_loss: 0.4557 train_acc: 0.8000\n",
      "Epoch 133 train_loss: 0.3809 train_acc: 0.8583\n",
      "Epoch 134 train_loss: 0.4780 train_acc: 0.8250\n",
      "Epoch 135 train_loss: 0.4434 train_acc: 0.8333\n",
      "Epoch 136 train_loss: 0.4355 train_acc: 0.8167\n",
      "Epoch 137 train_loss: 0.5129 train_acc: 0.7833\n",
      "Epoch 138 train_loss: 0.4296 train_acc: 0.8000\n",
      "Epoch 139 train_loss: 0.4677 train_acc: 0.7917\n",
      "Epoch 140 train_loss: 0.4503 train_acc: 0.8083\n",
      "Epoch 141 train_loss: 0.4018 train_acc: 0.8583\n",
      "Epoch 142 train_loss: 0.2791 train_acc: 0.8917\n",
      "Epoch 143 train_loss: 0.4290 train_acc: 0.8250\n",
      "Epoch 144 train_loss: 0.5800 train_acc: 0.7417\n",
      "Epoch 145 train_loss: 0.4198 train_acc: 0.8417\n",
      "Epoch 146 train_loss: 0.3753 train_acc: 0.8500\n",
      "Epoch 147 train_loss: 0.4561 train_acc: 0.8000\n",
      "Epoch 148 train_loss: 0.2661 train_acc: 0.8833\n",
      "Epoch 149 train_loss: 0.4383 train_acc: 0.8250\n",
      "Epoch 150 train_loss: 0.4744 train_acc: 0.8167\n",
      "Epoch 151 train_loss: 0.4390 train_acc: 0.8167\n",
      "Epoch 152 train_loss: 0.4215 train_acc: 0.8167\n",
      "Epoch 153 train_loss: 0.4246 train_acc: 0.8417\n",
      "Epoch 154 train_loss: 0.4558 train_acc: 0.8167\n",
      "Epoch 155 train_loss: 0.4341 train_acc: 0.8333\n",
      "Epoch 156 train_loss: 0.5266 train_acc: 0.7750\n",
      "Epoch 157 train_loss: 0.4191 train_acc: 0.8333\n",
      "Epoch 158 train_loss: 0.4205 train_acc: 0.8417\n",
      "Epoch 159 train_loss: 0.3328 train_acc: 0.8500\n",
      "Epoch 160 train_loss: 0.3920 train_acc: 0.8250\n",
      "Epoch 161 train_loss: 0.4166 train_acc: 0.8167\n",
      "Epoch 162 train_loss: 0.4163 train_acc: 0.8333\n",
      "Epoch 163 train_loss: 0.5116 train_acc: 0.7917\n",
      "Epoch 164 train_loss: 0.4486 train_acc: 0.7917\n",
      "Epoch 165 train_loss: 0.4819 train_acc: 0.8000\n",
      "Epoch 166 train_loss: 0.4381 train_acc: 0.8083\n",
      "Epoch 167 train_loss: 0.4201 train_acc: 0.8250\n",
      "Epoch 168 train_loss: 0.4893 train_acc: 0.8000\n",
      "Epoch 169 train_loss: 0.3295 train_acc: 0.8500\n",
      "Epoch 170 train_loss: 0.5737 train_acc: 0.7500\n",
      "Epoch 171 train_loss: 0.4036 train_acc: 0.8417\n",
      "Epoch 172 train_loss: 0.4557 train_acc: 0.8000\n",
      "Epoch 173 train_loss: 0.4166 train_acc: 0.8333\n",
      "Epoch 174 train_loss: 0.3851 train_acc: 0.8333\n",
      "Epoch 175 train_loss: 0.4820 train_acc: 0.8167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 176 train_loss: 0.4852 train_acc: 0.8000\n",
      "Epoch 177 train_loss: 0.4673 train_acc: 0.7833\n",
      "Epoch 178 train_loss: 0.3684 train_acc: 0.8417\n",
      "Epoch 179 train_loss: 0.3578 train_acc: 0.8500\n",
      "Epoch 180 train_loss: 0.3818 train_acc: 0.8500\n",
      "Epoch 181 train_loss: 0.4694 train_acc: 0.7833\n",
      "Epoch 182 train_loss: 0.3902 train_acc: 0.8583\n",
      "Epoch 183 train_loss: 0.4194 train_acc: 0.8000\n",
      "Epoch 184 train_loss: 0.5075 train_acc: 0.8000\n",
      "Epoch 185 train_loss: 0.3708 train_acc: 0.8583\n",
      "Epoch 186 train_loss: 0.4108 train_acc: 0.8083\n",
      "Epoch 187 train_loss: 0.5436 train_acc: 0.7583\n",
      "Epoch 188 train_loss: 0.2891 train_acc: 0.8667\n",
      "Epoch 189 train_loss: 0.4072 train_acc: 0.8083\n",
      "Epoch 190 train_loss: 0.5436 train_acc: 0.7583\n",
      "Epoch 191 train_loss: 0.4084 train_acc: 0.8333\n",
      "Epoch 192 train_loss: 0.4231 train_acc: 0.8333\n",
      "Epoch 193 train_loss: 0.3659 train_acc: 0.8333\n",
      "Epoch 194 train_loss: 0.3416 train_acc: 0.8583\n",
      "Epoch 195 train_loss: 0.2958 train_acc: 0.8667\n",
      "Epoch 196 train_loss: 0.3143 train_acc: 0.8583\n",
      "Epoch 197 train_loss: 0.4362 train_acc: 0.8000\n",
      "Epoch 198 train_loss: 0.3675 train_acc: 0.8417\n",
      "Epoch 199 train_loss: 0.4298 train_acc: 0.8083\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(200):\n",
    "    out = model(data)\n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    _, pred = torch.max(out[data.train_mask], dim=1)\n",
    "    correct = (pred == data.y[data.train_mask]).sum().item()\n",
    "    acc = correct/data.train_mask.sum().item()\n",
    "\n",
    "    print('Epoch {:03d} train_loss: {:.4f} train_acc: {:.4f}'.format(\n",
    "        epoch, loss.item(), acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 1.2906 test_acc: 0.6730\n"
     ]
    }
   ],
   "source": [
    "model.eval()path = \"/mnt/NVme/project_moka/pubmed/\"\n",
    "path = \"/mnt/ramfs/project_moka/pubmed/\"\n",
    "#path = \"/mnt/ext4ramdisk/project_moka/pubmed/\"\n",
    "#path = \"./pubmed/\"\n",
    "\n",
    "path_Cora = \"/mnt/NVme/project_moka/data/Cora/\"\n",
    "# path_Cora = \"/mnt/ramfs/project_moka/data/Cora/\"\n",
    "#path_Cora = \"/mnt/ext4ramdisk/project_moka/data/Cora/\"\n",
    "#path_Cora = \"./data/Cora/\"\n",
    "\n",
    "path_pm = \"/mnt/NVme/datasets/\"\n",
    "path_ram = \"/mnt/ramfs/datasets/\"\n",
    "\n",
    "times = 15\n",
    "total_time = 0\n",
    "total_run_time = 0\n",
    "batch_size = 128\n",
    "epoch_num = 20\n",
    "\n",
    "# pre-load Planetoid\n",
    "dataset_test = Planetoid(root=\"./pubmed/\", name='Pubmed')\n",
    "#dataset_test = Planetoid(root=path_Cora, name='Cora')\n",
    "# the dataset for test is shown below (different from the above)\n",
    "# dataset_test = Planetoid(root='./data/Cora/', name='Cora')\n",
    "\n",
    "\n",
    "dataset = dataset_test\n",
    "\n",
    "\n",
    "data = dataset[0]  # Get the first graph object.\n",
    "\n",
    "from torch_geometric.data import ClusterData, ClusterLoader, DataLoader\n",
    "\n",
    "torch.manual_seed(32322)\n",
    "cluster_data = ClusterData(data, num_parts=128)  # 1. Create subgraphs.\n",
    "train_loader = ClusterLoader(cluster_data, batch_size=batch_size,\n",
    "                             shuffle=True)  # 2. Stochastic partitioning scheme.\n",
    "out = model(data)\n",
    "loss = criterion(out[data.test_mask], data.y[data.test_mask])\n",
    "_, pred = torch.max(out[data.test_mask], dim=1)\n",
    "correct = (pred == data.y[data.test_mask]).sum().item()\n",
    "acc = correct/data.test_mask.sum().item()\n",
    "print(\"test_loss: {:.4f} test_acc: {:.4f}\".format(loss.item(), acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4271/3511194580.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'file.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mO_CREAT\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mO_TRUNC\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mO_DIRECT\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mO_RDWR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m: [Errno 22] Invalid argument"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "f = os.open('file.txt', os.O_CREAT | os.O_TRUNC | os.O_DIRECT | os.O_RDWR)\n",
    "s = ' ' * 1024\n",
    "os.write(f, s.encode())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
